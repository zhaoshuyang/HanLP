%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for hankcs at 2020-12-31 15:16:06 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{pennington-etal-2014-glove,
	address = {Doha, Qatar},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	date-added = {2020-12-31 15:07:29 -0500},
	date-modified = {2020-12-31 15:07:29 -0500},
	doi = {10.3115/v1/D14-1162},
	month = oct,
	pages = {1532--1543},
	publisher = {Association for Computational Linguistics},
	title = {{G}lo{V}e: Global Vectors for Word Representation},
	url = {https://www.aclweb.org/anthology/D14-1162},
	year = {2014},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D14-1162},
	Bdsk-Url-2 = {https://doi.org/10.3115/v1/D14-1162}}

@incollection{he2018dual,
	author = {He, Han and Wu, Lei and Yang, Xiaokun and Yan, Hua and Gao, Zhimin and Feng, Yi and Townsend, George},
	booktitle = {Information Technology-New Generations},
	date-added = {2020-12-31 15:03:58 -0500},
	date-modified = {2020-12-31 15:03:58 -0500},
	pages = {421--426},
	publisher = {Springer},
	title = {Dual long short-term memory networks for sub-character representation learning},
	year = {2018}}

@inproceedings{devlin-etal-2019-bert,
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	address = {Minneapolis, Minnesota},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	date-added = {2020-12-31 14:46:54 -0500},
	date-modified = {2020-12-31 14:46:54 -0500},
	doi = {10.18653/v1/N19-1423},
	month = jun,
	pages = {4171--4186},
	publisher = {Association for Computational Linguistics},
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://www.aclweb.org/anthology/N19-1423},
	year = {2019},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/N19-1423},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/N19-1423}}

@inproceedings{Lan2020ALBERT:,
	author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
	booktitle = {International Conference on Learning Representations},
	date-added = {2020-12-31 14:44:52 -0500},
	date-modified = {2020-12-31 14:44:52 -0500},
	title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	url = {https://openreview.net/forum?id=H1eA7AEtvS},
	year = {2020},
	Bdsk-Url-1 = {https://openreview.net/forum?id=H1eA7AEtvS}}

@inproceedings{wang-xu-2017-convolutional,
	abstract = {Character-based sequence labeling framework is flexible and efficient for Chinese word segmentation (CWS). Recently, many character-based neural models have been applied to CWS. While they obtain good performance, they have two obvious weaknesses. The first is that they heavily rely on manually designed bigram feature, i.e. they are not good at capturing $n$-gram features automatically. The second is that they make no use of full word information. For the first weakness, we propose a convolutional neural model, which is able to capture rich $n$-gram features without any feature engineering. For the second one, we propose an effective approach to integrate the proposed model with word embeddings. We evaluate the model on two benchmark datasets: PKU and MSR. Without any feature engineering, the model obtains competitive performance {---} 95.7{\%} on PKU and 97.3{\%} on MSR. Armed with word embeddings, the model achieves state-of-the-art performance on both datasets {---} 96.5{\%} on PKU and 98.0{\%} on MSR, without using any external labeled resource.},
	address = {Taipei, Taiwan},
	author = {Wang, Chunqi and Xu, Bo},
	booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2020-12-31 14:42:35 -0500},
	date-modified = {2020-12-31 14:42:35 -0500},
	month = nov,
	pages = {163--172},
	publisher = {Asian Federation of Natural Language Processing},
	title = {Convolutional Neural Network with Word Embeddings for {C}hinese Word Segmentation},
	url = {https://www.aclweb.org/anthology/I17-1017},
	year = {2017},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/I17-1017}}

@inproceedings{bertbaseline,
	author = {He, Han and Choi, Jinho},
	booktitle = {The Thirty-Third International Flairs Conference},
	date-added = {2020-12-31 14:32:43 -0500},
	date-modified = {2020-12-31 14:32:43 -0500},
	title = {Establishing Strong Baselines for the New Decade: Sequence Tagging, Syntactic and Semantic Parsing with BERT},
	year = {2020}}

@article{bojanowski2017enriching,
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	date-added = {2020-12-25 22:31:59 -0500},
	date-modified = {2020-12-25 22:31:59 -0500},
	issn = {2307-387X},
	journal = {Transactions of the Association for Computational Linguistics},
	pages = {135--146},
	title = {Enriching Word Vectors with Subword Information},
	volume = {5},
	year = {2017}}

@article{collins-koo-2005-discriminative,
	author = {Collins, Michael and Koo, Terry},
	date-added = {2020-12-25 17:25:59 -0500},
	date-modified = {2020-12-25 17:25:59 -0500},
	doi = {10.1162/0891201053630273},
	journal = {Computational Linguistics},
	number = {1},
	pages = {25--70},
	title = {Discriminative Reranking for Natural Language Parsing},
	url = {https://www.aclweb.org/anthology/J05-1003},
	volume = {31},
	year = {2005},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/J05-1003},
	Bdsk-Url-2 = {https://doi.org/10.1162/0891201053630273}}

@inproceedings{zhang-clark-2008-tale,
	address = {Honolulu, Hawaii},
	author = {Zhang, Yue and Clark, Stephen},
	booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2020-12-25 15:10:10 -0500},
	date-modified = {2020-12-25 15:10:10 -0500},
	month = oct,
	pages = {562--571},
	publisher = {Association for Computational Linguistics},
	title = {A Tale of Two Parsers: {I}nvestigating and Combining Graph-based and Transition-based Dependency Parsing},
	url = {https://www.aclweb.org/anthology/D08-1059},
	year = {2008},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D08-1059}}

@inproceedings{pradhan-etal-2012-conll,
	address = {Jeju Island, Korea},
	author = {Pradhan, Sameer and Moschitti, Alessandro and Xue, Nianwen and Uryupina, Olga and Zhang, Yuchen},
	booktitle = {Joint Conference on {EMNLP} and {C}o{NLL} - Shared Task},
	date-added = {2020-12-24 23:42:41 -0500},
	date-modified = {2020-12-24 23:42:41 -0500},
	month = jul,
	pages = {1--40},
	publisher = {Association for Computational Linguistics},
	title = {{C}o{NLL}-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in {O}nto{N}otes},
	url = {https://www.aclweb.org/anthology/W12-4501},
	year = {2012},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W12-4501}}

@inproceedings{levow-2006-third,
	address = {Sydney, Australia},
	author = {Levow, Gina-Anne},
	booktitle = {Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing},
	date-added = {2020-12-24 23:21:14 -0500},
	date-modified = {2020-12-24 23:21:14 -0500},
	month = jul,
	pages = {108--117},
	publisher = {Association for Computational Linguistics},
	title = {The Third International {C}hinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition},
	url = {https://www.aclweb.org/anthology/W06-0115},
	year = {2006},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W06-0115}}

@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,
	author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
	booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003},
	date-added = {2020-12-24 23:19:00 -0500},
	date-modified = {2020-12-24 23:19:00 -0500},
	pages = {142--147},
	title = {Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
	url = {https://www.aclweb.org/anthology/W03-0419},
	year = {2003},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W03-0419}}

@inproceedings{koehn2005europarl,
	author = {Koehn, Philipp},
	booktitle = {MT summit},
	date-added = {2020-12-24 23:06:03 -0500},
	date-modified = {2020-12-24 23:06:03 -0500},
	organization = {Citeseer},
	pages = {79--86},
	title = {Europarl: A parallel corpus for statistical machine translation},
	volume = {5},
	year = {2005}}

@inproceedings{Schweter:Ahmed:2019,
	author = {Stefan Schweter and Sajawel Ahmed},
	booktitle = {Proceedings of the 15th Conference on Natural Language Processing (KONVENS)},
	date-added = {2020-12-24 23:03:23 -0500},
	date-modified = {2020-12-24 23:03:23 -0500},
	location = {Erlangen, Germany},
	note = {accepted},
	title = {{Deep-EOS: General-Purpose Neural Networks for Sentence Boundary Detection}},
	year = 2019}

@incollection{he2019effective,
	author = {He, Han and Wu, Lei and Yan, Hua and Gao, Zhimin and Feng, Yi and Townsend, George},
	booktitle = {Smart Intelligent Computing and Applications},
	date-added = {2020-12-24 19:35:03 -0500},
	date-modified = {2020-12-24 19:35:03 -0500},
	pages = {133--142},
	publisher = {Springer},
	title = {Effective neural solution for multi-criteria word segmentation},
	year = {2019}}

@inproceedings{dozat2017stanford,
	author = {Dozat, Timothy and Qi, Peng and Manning, Christopher D},
	booktitle = {Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
	date-added = {2020-12-24 15:02:18 -0500},
	date-modified = {2020-12-24 15:02:18 -0500},
	pages = {20--30},
	title = {Stanford's graph-based neural dependency parser at the conll 2017 shared task},
	year = {2017}}

@inproceedings{he-etal-2018-jointly,
	abstract = {Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.},
	address = {Melbourne, Australia},
	author = {He, Luheng and Lee, Kenton and Levy, Omer and Zettlemoyer, Luke},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2020-12-24 14:23:45 -0500},
	date-modified = {2020-12-24 14:23:45 -0500},
	doi = {10.18653/v1/P18-2058},
	month = jul,
	pages = {364--369},
	publisher = {Association for Computational Linguistics},
	title = {Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling},
	url = {https://www.aclweb.org/anthology/P18-2058},
	year = {2018},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/P18-2058},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/P18-2058}}

@inproceedings{yu-etal-2020-named,
	abstract = {Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.},
	address = {Online},
	author = {Yu, Juntao and Bohnet, Bernd and Poesio, Massimo},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2020-12-24 13:35:09 -0500},
	date-modified = {2020-12-24 13:35:09 -0500},
	doi = {10.18653/v1/2020.acl-main.577},
	month = jul,
	pages = {6470--6476},
	publisher = {Association for Computational Linguistics},
	title = {Named Entity Recognition as Dependency Parsing},
	url = {https://www.aclweb.org/anthology/2020.acl-main.577},
	year = {2020},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/2020.acl-main.577},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/2020.acl-main.577}}

@inproceedings{10.1145/1457838.1457895,
	abstract = {Many computer applications require the storage of large amounts of information within the computer's memory where it will be readily available for reference and updating. Quite commonly, more storage space is required than is available in the computer's high-speed working memory. It is, therefore, a common practice to equip computers with magnetic tapes, disks, or drums, or a combination of these to provide additional storage. This additional storage is always slower in operation than the computer's working memory and therefore care must be taken when using it to avoid excessive operating time.},
	address = {New York, NY, USA},
	author = {De La Briandais, Rene},
	booktitle = {Papers Presented at the the March 3-5, 1959, Western Joint Computer Conference},
	date-added = {2020-12-24 13:07:31 -0500},
	date-modified = {2020-12-24 13:07:31 -0500},
	doi = {10.1145/1457838.1457895},
	isbn = {9781450378659},
	location = {San Francisco, California},
	numpages = {4},
	pages = {295--298},
	publisher = {Association for Computing Machinery},
	series = {IRE-AIEE-ACM '59 (Western)},
	title = {File Searching Using Variable Length Keys},
	url = {https://doi.org/10.1145/1457838.1457895},
	year = {1959},
	Bdsk-Url-1 = {https://doi.org/10.1145/1457838.1457895}}

@article{lafferty2001conditional,
	author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando CN},
	date-added = {2020-12-24 11:46:30 -0500},
	date-modified = {2020-12-24 12:08:29 -0500},
	journal = {Departmental Papers (CIS)},
	title = {Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
	year = {2001}}

@inproceedings{clark-etal-2019-bam,
	abstract = {It can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts. To help address this, we propose using knowledge distillation where single-task models teach a multi-task model. We enhance this training with teacher annealing, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers. We evaluate our approach by multi-task fine-tuning BERT on the GLUE benchmark. Our method consistently improves over standard single-task and multi-task training.},
	address = {Florence, Italy},
	author = {Clark, Kevin and Luong, Minh-Thang and Khandelwal, Urvashi and Manning, Christopher D. and Le, Quoc V.},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2020-12-24 11:26:54 -0500},
	date-modified = {2020-12-24 11:26:54 -0500},
	doi = {10.18653/v1/P19-1595},
	month = jul,
	pages = {5931--5937},
	publisher = {Association for Computational Linguistics},
	title = {{BAM}! Born-Again Multi-Task Networks for Natural Language Understanding},
	url = {https://www.aclweb.org/anthology/P19-1595},
	year = {2019},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/P19-1595},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/P19-1595}}

@inproceedings{kondratyuk-straka-2019-75,
	address = {Hong Kong, China},
	author = {Kondratyuk, Dan and Straka, Milan},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2020-12-23 23:51:07 -0500},
	date-modified = {2020-12-23 23:51:07 -0500},
	pages = {2779--2795},
	publisher = {Association for Computational Linguistics},
	title = {75 Languages, 1 Model: Parsing Universal Dependencies Universally},
	url = {https://www.aclweb.org/anthology/D19-1279},
	year = {2019},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D19-1279}}

@inproceedings{dozat:17a,
	author = {Dozat, Timothy and Manning, Christopher D.},
	booktitle = {Proceedings of the 5th International Conference on Learning Representations},
	date-added = {2020-12-23 23:46:20 -0500},
	date-modified = {2020-12-23 23:46:20 -0500},
	series = {ICLR'17},
	title = {{Deep Biaffine Attention for Neural Dependency Parsing}},
	url = {https://openreview.net/pdf?id=Hk95PK9le},
	year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1611.01734},
	Bdsk-Url-2 = {https://openreview.net/pdf?id=Hk95PK9le}}

@inproceedings{smith-smith-2007-probabilistic,
	address = {Prague, Czech Republic},
	author = {Smith, David A. and Smith, Noah A.},
	booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})},
	date-added = {2020-12-23 21:46:06 -0500},
	date-modified = {2020-12-23 21:46:06 -0500},
	month = jun,
	pages = {132--140},
	publisher = {Association for Computational Linguistics},
	title = {Probabilistic Models of Nonprojective Dependency Trees},
	url = {https://www.aclweb.org/anthology/D07-1014},
	year = {2007},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D07-1014}}

@inproceedings{ijcai2020-560,
	author = {Zhang, Yu and Zhou, Houquan and Li, Zhenghua},
	booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI-20}},
	date-added = {2020-12-23 21:36:56 -0500},
	date-modified = {2020-12-23 21:36:56 -0500},
	doi = {10.24963/ijcai.2020/560},
	editor = {Christian Bessiere},
	month = {7},
	note = {Main track},
	pages = {4046--4053},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {Fast and Accurate Neural CRF Constituency Parsing},
	url = {https://doi.org/10.24963/ijcai.2020/560},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.24963/ijcai.2020/560}}

@inproceedings{buchholz-marsi-2006-conll,
	address = {New York City},
	author = {Buchholz, Sabine and Marsi, Erwin},
	booktitle = {Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X)},
	date-added = {2020-12-22 22:57:41 -0500},
	date-modified = {2020-12-22 22:57:41 -0500},
	month = jun,
	pages = {149--164},
	publisher = {Association for Computational Linguistics},
	title = {{C}o{NLL}-{X} Shared Task on Multilingual Dependency Parsing},
	url = {https://www.aclweb.org/anthology/W06-2920},
	year = {2006},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W06-2920}}
